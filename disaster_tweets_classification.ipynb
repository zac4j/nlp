{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":6068,"sourceType":"modelInstanceVersion","modelInstanceId":4689,"modelId":2821}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dazhengzhu/nlp-with-disaster-tweets?scriptVersionId=194661528\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Natural Language Processing with Disaster Tweets ","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nThis project is to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. A dataset of 10,000 tweets that were hand classified is available.\n\nBefore we train a model on text data, we need to preprocess the text. In many cases, text needs to be **tokenized** and **vectorized** before it can be fed to a model, and in some cases the text requires additional preprocessing steps such as **normalization** and **feature selection**. After text is processed into a suitable format, we use it in natural language processing (NLP) workflow.\n\nIn this project we build two models for comparision: \n- Baseline model: The recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\n- SOTA model: The KerasNLP is a natural language processing library that supports workflows built from modular components that have state-of-the-art preset weights and architectures.\n\nFinally, according to the performance of the model, we select the best model to make prediction.","metadata":{}},{"cell_type":"markdown","source":"### Key Takeaways\n\nKerasNLP.DistilBERT model outperforms RNN model for disaster tweet classification, the findings include:\n\n* **Higher Accuracy:** DistilBERT achieved a higher accuracy rate, indicating that it was more effective in correctly classifying disaster tweets.\n* **Lower Loss:** The DistilBERT model exhibited a lower loss value, suggesting that it made fewer errors during training and testing.\n* **Longer Training Time:** While DistilBERT required more time to train, its improved performance justified the increased computational cost.","metadata":{}},{"cell_type":"markdown","source":"### Setup","metadata":{}},{"cell_type":"markdown","source":"Set up Keras environment:","metadata":{}},{"cell_type":"code","source":"# This sample uses Keras Core, the multi-backend version of Keras.\n# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\nimport os\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n\n# !pip install keras_core\n# !pip install keras_nlp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import Tensorflow, Keras and other necessary libearies:","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport string\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data","metadata":{}},{"cell_type":"markdown","source":"#### Preview the dataset","metadata":{}},{"cell_type":"code","source":"TRAINING_FILE_PATH = '../input/nlp-getting-started/train.csv'\nTESTING_FILE_PATH = '../input/nlp-getting-started/test.csv'\n\ndf_train = pd.read_csv(TRAINING_FILE_PATH)\nprint(f'Training dataset:')\ntrain_shape = df_train.shape\ntrain_num_rows = train_shape[0]\nprint(f'Shape = {train_shape}')\nprint('Memory usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Head of data table:')\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at the train dataset.\n\n- id: Tweet id in the table.\n- keyword: A keyword from that tweet (may be blank)\n- location: The location the tweet was sent from (may be blank)\n- text: The text content of a tweet\n- target: 1 if the tweet is a real disaster or 0 if not","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(TESTING_FILE_PATH)\nprint(f'Testing dataset:')\ntest_shape = df_test.shape\n# Save test rows for inference\ntest_num_rows = test_shape[0]\nprint(f'Shape = {test_shape}')\nprint('Memory usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))\nprint('Head of data table:')\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see there are 7613 records for training, the `text` column contains the tweet content, and the corresponding label in the `target` column. Also, there're 3263 tweets for testing.","metadata":{}},{"cell_type":"markdown","source":"### Load dataset\n\nWe'll use the `tf.data.experimental.make_csv_dataset` to load the csv data, and shuffle the data and create batches of these data. Note that we set `num_epochs=1`, this ensure the dataset only goes through the data once, which making it finite, if this property was not been set, **the `vectorizer.adapt` process will be blocked.**\n\nThen we use given validation split ratio `VALIDATION_SPILT` to split the main dataset to testing & validation subset.","metadata":{}},{"cell_type":"code","source":"BUFFER_SIZE = 10000\nBATCH_SIZE = 64\nVALIDATION_SPLIT = 0.2\n\nmain_ds = tf.data.experimental.make_csv_dataset(\n    TRAINING_FILE_PATH,\n    select_columns=['text','target'],\n    label_name='target',\n    batch_size=BATCH_SIZE,\n    num_epochs=1, # Ensure go through the data once to create a finite dataset\n    shuffle=True,\n    shuffle_buffer_size=BUFFER_SIZE,\n    prefetch_buffer_size=tf.data.AUTOTUNE,\n    num_rows_for_inference=train_num_rows # Help tf optimize the dataset creation\n)\n\n# Spilt the main dataset to train & validation datasets\nnum_val_elements = int(train_num_rows * VALIDATION_SPLIT // BATCH_SIZE)\nraw_val_ds = main_ds.take(num_val_elements)\nraw_train_ds = main_ds.skip(num_val_elements)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"### Preview the dataset spec","metadata":{}},{"cell_type":"code","source":"raw_train_ds.element_spec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see each element is a `(dict, label)` tuple, and the `dict` contains `text` data.","metadata":{}},{"cell_type":"markdown","source":"### Preview tweets\n\nLet's preview first 3 tweets, notice the data order is shuffled:","metadata":{}},{"cell_type":"code","source":"for batch, label in raw_train_ds.take(1):\n    for key, value in batch.items():\n        print(f\"{key:20s}: {value[:3]}\")\n    print()\n    print(f\"{'label':20s}: {label[:3]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis Tweets\n\nBefore analysis tweets, we need to perform the text preprocessing, this step is to transform raw text data into a structured format suitable for analysis or modeling, by applying techniques like tokenization, stemming, stopwords removal, and normalization, we aim to facilitate analysis, reduce dimensionality, and enhance model performance.","metadata":{}},{"cell_type":"code","source":"def load_stopwords():\n  nltk.download('punkt')\n  nltk.download('stopwords')\n\nload_stopwords()\n\n# Function to preprocess text\ndef preprocess_text(text):\n  # Tokenization\n  words = nltk.word_tokenize(text=text, language='english')\n\n  # Remove stop words\n  stop_words = set(stopwords.words('english'))\n  words = [word for word in words if word not in stop_words]\n\n  # Stemming\n  stemmer = PorterStemmer()\n  words = [stemmer.stem(word) for word in words]\n\n  return ' '.join(words)\n\n# Apply preprocessing to the 'clean_text' column\ndf_train['clean_text'] = df_train['text'].apply(preprocess_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Show tweets Word Cloud","metadata":{}},{"cell_type":"code","source":"text = ' '.join(df_train['clean_text'])\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could observe from the Word Could picture:\n\n- High-frequency disaster words: words like \"help\", \"death\", \"life\", \"people\", \"survive\", \"kill\" are common used function words in English.\n- Specific disaster: words like \"flood\", \"drown\", \"fire\", \"storm\", \"bomb\" are consider specific disaster.","metadata":{}},{"cell_type":"markdown","source":"#### Show tweets Word Frequency","metadata":{}},{"cell_type":"code","source":"def get_word_counts(text_data):\n  all_words = []\n  for text in text_data:\n    # Remove punctuation marks\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    words = text.split()\n    all_words.extend(words) \n  word_counts = Counter(all_words)\n  word_freq = pd.DataFrame({'word': list(word_counts.keys()), 'count': list(word_counts.values())})\n  word_freq = word_freq.sort_values(by='count', ascending=False)\n  return word_freq\n\nword_freq = get_word_counts(df_train['clean_text'])\nword_freq.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on above Word Frequency data, the observations are:\n\n- Common function words: words likes \"I\", \"s\", \"the\", \"nt\", \"A\", \"m\" are common function words used in language.\n- Disaster specifics: words like \"fire\", \"burn\", \"kill\", and \"bomb\" suggest a focus on disaster specific.","metadata":{}},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#### Convert to text-only dataset\n\nAs the first preprocessing step, we'll need to convert the data type of dataset element as follows:","metadata":{}},{"cell_type":"code","source":"def dict_to_text(dict, label):\n  return dict['text'], label\n\ntrain_ds = raw_train_ds.map(dict_to_text)\nval_ds = raw_val_ds.map(dict_to_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the TextVectorization layer\n\nWe will create a `TextVectorization` layer, this layer will be used to standardize, tokennize, and vectorize our data. We also set the output_mode to int to create unique integer indices for each token.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\n\nMAX_FEATURES = 10000\nSEQUENCE_LENGTH = 160\n\nvectorizer = layers.TextVectorization(\n    max_tokens=MAX_FEATURES,\n    output_mode='int',\n    output_sequence_length=SEQUENCE_LENGTH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll call the `adapt` to fit the state of the preprocessing layer to the text-only dataset. This will cause the model to build an index of strings to integers.","metadata":{}},{"cell_type":"code","source":"# Make text-only dataset before adapt\ntext_ds = train_ds.map(lambda text, label:text)\nvectorizer.adapt(text_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding `''` and unknown `[UNK]` tokens are sorted by frequency:","metadata":{}},{"cell_type":"code","source":"vocabulary = np.array(vectorizer.get_vocabulary())\nvocabulary[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the vocabulary is set, the layer can vectorize text into indices, let's create a function to verctorize text:","metadata":{}},{"cell_type":"code","source":"def vectorize_text(text, label):\n  text = tf.expand_dims(text, -1)\n  return vectorizer(text), label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# retrieve a batch (of 64 texts and labels) from the dataset\ntexts, labels = next(iter(train_ds))\ntext1, label1 = texts[0], labels[0]\nvectorized_text, label = vectorize_text(text1, label1)\nprint(\"Original tweet: \", text1.numpy())\nprint(\"Label: \", label1.numpy())\nprint(\"Vectorized tweet: \", vectorized_text.numpy())\nprint(\"Round-trip tweet: \", \" \".join(vocabulary[vectorized_text.numpy()[0]]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the tensor of indices are 0-padded to the longest sequence in the batch.","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"### Build a Baseline Model\n\nWe'll build a recurrent neural network (RNN), the layers are stacked sequentially to build the classifier:\n\n- The first layer is the `vectorizer`, which converts the text to a sequence of token indices.\n\n- After the `vectorizer` is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n\n    This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `layers.Dense` layer.\n\n- The RNN processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n\n    The `layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output.\n\n    - The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.\n\n    - The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n\n- After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output.\n","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    vectorizer,\n    layers.Embedding(\n        input_dim=len(vectorizer.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    layers.Bidirectional(layers.LSTM(64,  return_sequences=True)),\n    layers.Bidirectional(layers.LSTM(32)),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1)\n])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The embedding layer uses masking to handle the varying sequence-lengths. All the layers after the Embedding support masking:","metadata":{}},{"cell_type":"code","source":"print([layer.supports_masking for layer in model.layers])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loss function and optimizer\n\nSince this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use `losses.BinaryCrossentropy` loss function.","metadata":{}},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the model","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\nrnn_history = model.fit(\n    train_ds,\n    validation_data = val_ds,\n    epochs = EPOCHS\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build a SOTA Model\n\nWe'll choose the DistilBERT model, this model learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)). \n\nIt has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n\nSpecifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT.","metadata":{}},{"cell_type":"markdown","source":"#### Create preprocessor and classifier\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n\nThe BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().","metadata":{}},{"cell_type":"code","source":"# Load a DistilBERT model.\npreset= \"distil_bert_base_en_uncased\"\n\n# Use a shorter sequence length.\npreprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n                                                                   sequence_length=SEQUENCE_LENGTH,\n                                                                   name=\"preprocessor_4_tweets\"\n                                                                  )\n\n# Pretrained classifier.\nclassifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n                                                               preprocessor = preprocessor, \n                                                               num_classes=2)\n\nclassifier.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Compile classifier\n\nFor the NLP classifier, we'll use `losses.SparseCategoricalCrossentropy` loss function.","metadata":{}},{"cell_type":"code","source":"classifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(1e-4),\n    metrics= [\"accuracy\"]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the classifier","metadata":{}},{"cell_type":"code","source":"nlp_history = classifier.fit(train_ds,\n                         epochs=10, \n                         validation_data=val_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results and analysis\n\nIn this section, we'll evaluate above created models.","metadata":{}},{"cell_type":"markdown","source":"### Evaluate the baseline model\n\nLet's see how the model performs.","metadata":{}},{"cell_type":"code","source":"loss, accuracy = model.evaluate(val_ds)\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The baseline model achieves an accuracy of about 95%, this indicates a good result.","metadata":{}},{"cell_type":"markdown","source":"#### Preview learning curve\n\nBefore show the learning curve, let's take a look at the `rnn_history` object:","metadata":{}},{"cell_type":"code","source":"rnn_history.history.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then let's preview the baseline model learning curve:","metadata":{}},{"cell_type":"code","source":"def preview_learning_curve(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.ylim([min(plt.ylim()),1])\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Cross Entropy')\n    plt.ylim([0,1.0])\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n\npreview_learning_curve(rnn_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training loss decreases with each epoch and the training accuracy increases with each epoch, the loss and accuracy of validation performs similarly, these are expected behaves.","metadata":{}},{"cell_type":"markdown","source":"### Evaluate the sota model","metadata":{}},{"cell_type":"markdown","source":"#### Preview learning curve\n\n","metadata":{}},{"cell_type":"code","source":"preview_learning_curve(nlp_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training & validation loss decreases with each epoch and the training & validation accuracy increases with each epoch, compare with the baseline model, the nlp model achieves higher accuracy, lower loss, which reflects its more advanced strength. We'll select this model to make the prediction.","metadata":{}},{"cell_type":"markdown","source":"### Submission\n\nFor each tweets in the test set, we predict if the given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\nThe `submission.csv` file uses the following format:\n`id,target`","metadata":{}},{"cell_type":"markdown","source":"#### Prepare the Test Dataset","metadata":{}},{"cell_type":"code","source":"raw_test_ds = tf.data.experimental.make_csv_dataset(\n    TESTING_FILE_PATH,\n    select_columns=['id','text'],\n    batch_size=BATCH_SIZE,\n    num_epochs=1, # Ensure go through the data once to create a finite dataset\n    shuffle=False, # No shuffle for testing data\n    prefetch_buffer_size=tf.data.AUTOTUNE,\n    num_rows_for_inference=test_num_rows # Help tf optimize the dataset creation\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = raw_test_ds.map(lambda batch: batch['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Make prediction","metadata":{}},{"cell_type":"markdown","source":"Predict by RNN model","metadata":{}},{"cell_type":"code","source":"# Comment out to speed up notebook execution\n# rnn_predictions = model.predict(test_ds)\n# rnn_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict by NLP model","metadata":{}},{"cell_type":"code","source":"nlp_predictions = classifier.predict(test_ds)\nnlp_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert predictions to labels","metadata":{}},{"cell_type":"markdown","source":"For the rnn model prediction, the results have single column array, and if prediction >= 0.0, the label is 1 else it's 0.","metadata":{}},{"cell_type":"code","source":"# Comment out to speed up\n# This line used for convert rnn model predictions\n# rnn_labels = np.where(predictions >= 0, 1, 0)\n# rnn_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the nlp model prediction, the results have two columns, for each row, if the first column value is larger, the label is 0, else it's 1.","metadata":{}},{"cell_type":"code","source":"nlp_labels = np.argmax(nlp_predictions, axis=1)\nnlp_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Save submission","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This line\nsubmission['target'] = nlp_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nBased on the experimental results, DistilBERT (Transformer) consistently demonstrated superior performance compared to the RNN model in the disaster tweet classification task.\n\nThe superior performance of DistilBERT can be attributed to its pre-trained nature and ability to capture complex language patterns. DistilBERT was pre-trained on a massive amount of text data, allowing it to learn contextual relationships between words and phrases. This pre-training enables DistilBERT to extract meaningful features from disaster tweets, leading to more accurate classifications.\n\nIn conclusion, DistilBERT is a promising approach for disaster tweet classification tasks. Its ability to achieve higher accuracy and lower loss, despite the increased training time, makes it a valuable tool for natural language processing applications in crisis management and information retrieval.\n","metadata":{}},{"cell_type":"markdown","source":"## Reference\n\n- [Recurrent neural network](https://aws.amazon.com/what-is/recurrent-neural-network)\n- [Google: Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification)\n- [Tensorflow: Working with RNNs](https://www.tensorflow.org/guide/keras/working_with_rnns)\n- [Tensorflow: Text classification with an RNN](https://www.tensorflow.org/text/tutorials/text_classification_rnn)\n- [Tensorflow: Compute Similarity Metrics](https://www.tensorflow.org/text/tutorials/text_similarity)\n- [Tensorflow: Basic text classification with Keras](https://www.tensorflow.org/tutorials/keras/text_classification)\n- [Tensorflow: Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)\n- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter (NeurIPS 2019)](https://arxiv.org/abs/1910.01108)","metadata":{}}]}